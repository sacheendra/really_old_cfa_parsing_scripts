{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask.delayed import delayed\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from PySquashfsImage import SquashFsImage\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "import shutil\n",
    "from dask.distributed import get_worker\n",
    "import re\n",
    "import itertools\n",
    "from dateutil.tz import gettz\n",
    "from bs4 import BeautifulSoup\n",
    "import dateutil\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "from dateutil import parser as dateparser\n",
    "import string\n",
    "from itertools import zip_longest\n",
    "from scipy.special import comb\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tzinfos = {\n",
    "    'PST': gettz('Etc/GMT-8'),\n",
    "    'PDT': gettz('Etc/GMT-7'),\n",
    "    # https://www.timeanddate.com/time/zones/est\n",
    "    'EST': gettz('Etc/GMT-5'),\n",
    "    # https://www.timeanddate.com/time/zones/edt\n",
    "    'EDT': gettz('Etc/GMT-4'),\n",
    "    # https://www.timeanddate.com/time/zones/cest\n",
    "    'CET': gettz('Etc/GMT+1')\n",
    "}\n",
    "\n",
    "def process_downdetector_file(filename, file_obj, metadata):\n",
    "    service_name = filename.split('.')[0]\n",
    "    html = str(file_obj.read())\n",
    "    # pprint(html)\n",
    "    if html is None:\n",
    "        raise ValueError(f'ERROR: HTML IS NONE \\n filename {filename} \\t file_obj {file_obj} ')\n",
    "    json_lines = []\n",
    "\n",
    "    json_key_value_regex = \\\n",
    "        r\"{{1}\\s(\\w)+:\\s+\\\\'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{6}\\+\\d{2}:\\d{2}\\\\',\\s+\\w+:\\s+\\d+\\s+}{1},\"\n",
    "    json_lines = [x.group() for x in re.finditer(json_key_value_regex, html)]\n",
    "    dates_values = [x.replace('\\\\', '').split(',') for x in json_lines]\n",
    "\n",
    "    # TODO make a better regex\n",
    "    date_rgx = re.compile(r\"[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\\.[0-9]{6}\\+[0-9]{2}:[0-9]{2}\")\n",
    "    value_rgx = re.compile(r\"[0-9]+\")\n",
    "\n",
    "    date_list, value_list = [], []\n",
    "    for dv in dates_values:\n",
    "        if date_rgx.search(dv[0]) is not None:\n",
    "            date_list.append(date_rgx.search(dv[0]).group(0))\n",
    "        else:\n",
    "            raise ValueError(f'ERROR: Regex failed to find timestamp -> '\n",
    "                             f'filename: {filename} \\t file_obj {file_obj}\\n ')\n",
    "\n",
    "        if value_rgx.search(dv[1]) is not None:\n",
    "            value_list.append(value_rgx.search(dv[1]).group(0))\n",
    "        else:\n",
    "            raise ValueError(f'ERROR:  Regex failed to find severity-> '\n",
    "                             f'filename: {filename} \\t file_obj {file_obj}\\n ')\n",
    "\n",
    "    if len(date_list) != len(value_list):\n",
    "        raise ValueError(f'ERROR: Length of lists dont match \\n '\n",
    "                         f'date_list length:{len(date_list)} must equal value_list length:{len(value_list)}\\n'\n",
    "                         f'filename {filename} \\t file_obj {file_obj} ')\n",
    "    # pprint(date_list)\n",
    "\n",
    "    # timeseries = [parser.parse(date, tzinfos=tzinfos) for date in date_list]\n",
    "\n",
    "    return pd.DataFrame({'event_time': [dateparser.parse(date, tzinfos=tzinfos) for date in date_list],\n",
    "                         'status_code': value_list,\n",
    "#                          'request_time': dateparser.parse(f'{existing_parts[2]} EST', tzinfos=tzinfos),\n",
    "                         'status_info_updated': None,  # TODO: Missing\n",
    "                         'last_status_change': None,  # TODO: Missing\n",
    "                         'last_w_service_disruption': None,  # TODO: Missing\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRNParser:\n",
    "\n",
    "    def time_since_timestart(self, elem):\n",
    "        return int(1440 * elem)  # 24 * 60 = 1440\n",
    "\n",
    "\n",
    "    def get_availability(self, code):\n",
    "        status = {\n",
    "            83: 1.0,  # 'Up',\n",
    "            66: 0.8,  # 'Recent Signs of Service Trouble',\n",
    "            49: 0.6,  # 'Possible Service Trouble',\n",
    "            33: 0.4,  # 'Likely Service Disruption',\n",
    "            16: 0  # 'Confirmed Service Disruption'\n",
    "        }\n",
    "        return status.get(code)\n",
    "\n",
    "\n",
    "    def decode64(self, raw_string):\n",
    "        allowed_characters = string.ascii_uppercase + string.ascii_lowercase + string.digits + '-.__'\n",
    "        chr_groups = [\n",
    "            (m, n or '__')\n",
    "            for m, n\n",
    "            in list(zip_longest(raw_string, raw_string[1:]))[::2]\n",
    "        ]\n",
    "        return [\n",
    "            (allowed_characters.index(m) * 64 + allowed_characters.index(n)) / 4095\n",
    "            for m, n\n",
    "            in chr_groups\n",
    "        ]\n",
    "\n",
    "\n",
    "    def trans_service_status(self, series):\n",
    "        return [self.get_availability(int(s * 100)) for s in series]\n",
    "\n",
    "\n",
    "    # 159451   2016-01-31 23:37:00-02:00  1.0 yahoomail    2016-01-31 23:37:00-02:00\n",
    "    # 2013-01-24 21:37+02:00\n",
    "    # ONLY WORKS IF THE FILE IS SOURCED FROM CLOUD AVAILABILITY ARCHIVE -> filetime\n",
    "    def trans_time_series(self, series, filetime):\n",
    "        time_series_data = []\n",
    "\n",
    "        # turn the clock back by one day since the chart is 24 hours long\n",
    "        timestart = filetime - datetime.timedelta(days=1)\n",
    "        for inx, elem in enumerate(list(series)):\n",
    "            if inx == 0:  # timeseries[0] begins is timestart\n",
    "                time_series_data.append(timestart)\n",
    "            else:\n",
    "                time_since_start = timestart + datetime.timedelta(minutes=self.time_since_timestart(elem))\n",
    "                time_series_data.append(time_since_start)\n",
    "        # pp.pprint(time_series_data)\n",
    "        return time_series_data\n",
    "\n",
    "\n",
    "    def process_file(self, filename, file_obj, scrape_time):\n",
    "        html = str(file_obj.read())\n",
    "        if html is None:\n",
    "            raise ValueError(f'ERROR: HTML IS NONE \\n filename {filename} \\t file_obj {file_obj} ')\n",
    "        if html == '':\n",
    "            # WARNING!!! IGNORING ERRORS. AFAIK, FEW EXIST. THIS DATA SHOULD BE DUPLICATED IN OTHER FILES.\n",
    "            return pd.DataFrame({'event_time': [datetime.datetime.now()],\n",
    "                                 'status_code': [-1]})\n",
    "            \n",
    "        try:\n",
    "            series_reg = re.compile(\"chd=e:([^&]{0,})\")\n",
    "            series_tuple = series_reg.findall(html)\n",
    "            if (len(series_tuple[0]) - 1) % 2 != 0:\n",
    "                print(html)\n",
    "                raise ValueError('Needs to be even')\n",
    "\n",
    "            splits = series_tuple[0].split(',')\n",
    "            enc_timeseries, enc_service_status = splits[0], splits[1]\n",
    "    #         filetime = list(existing_parts[5][:-4])\n",
    "            filetime = list(scrape_time)\n",
    "            filetime[8], filetime[11] = ' ', ':'  # cant parse the format w/ the standard library\n",
    "            filetime.append(' EST') # All data should be parsed as EST since it was scraped in N. Virginia\n",
    "            file_datetime = dateparser.parse(''.join(filetime[:-1]), tzinfos=tzinfos)\n",
    "            timeseries = self.trans_time_series(self.decode64(enc_timeseries), file_datetime)\n",
    "            service_status = self.trans_service_status(self.decode64(enc_service_status))\n",
    "\n",
    "            timestamp_reg = re.compile(\"timestamp_\\\\d{0,}\\\"\\\\>([^\\\\</]{0,})\")\n",
    "            all_timestamps = timestamp_reg.findall(html)\n",
    "            new_timestamps = [dateparser.parse(ma1, tzinfos=tzinfos) for ma1 in all_timestamps]\n",
    "        except Exception as e:\n",
    "            print(html)\n",
    "            raise e\n",
    "\n",
    "        return pd.DataFrame({'event_time': timeseries,\n",
    "                             'status_code': service_status})\n",
    "#                              'status_info_updated': new_timestamps[0],\n",
    "#                              'last_status_change': new_timestamps[1],\n",
    "#                              'last_w_service_disruption': new_timestamps[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutageReportParser:\n",
    "\n",
    "    def process_outagereport(self, filename, file_obj):\n",
    "        html = str(file_obj.read())\n",
    "        if html is None:\n",
    "            raise ValueError(f'ERROR: HTML IS NONE \\n filename {filename} \\t file_obj {file_obj} ')\n",
    "\n",
    "        matches = re.search('\"chartData\":(.*?)],', html)\n",
    "        statuses = []\n",
    "        if matches is not None:\n",
    "            statuses = json.loads(matches.group(1) + ']') # We used the closing brackets in the regex and don't capture them, adding them back again\n",
    "        \n",
    "        event_times = []\n",
    "        people_reporting = []\n",
    "#         eastern = pytz.timezone('US/Eastern')\n",
    "        for datapoint in statuses:\n",
    "            event_times.append(datetime.datetime.utcfromtimestamp(datapoint['ts']))\n",
    "            people_reporting.append(datapoint['count'])\n",
    "\n",
    "        return pd.DataFrame({'event_time': event_times,\n",
    "                             'status_code': people_reporting})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_TEMP_PREFIX = 'ctzph-01-'\n",
    "pathSplitRegex = re.compile('[./]')\n",
    "PLACEHOLDER_DF = pd.DataFrame([{\n",
    "                    'event_time': datetime.datetime.now(),\n",
    "                    'status_code': np.int32(-1),\n",
    "                    # 'request_time': dateparser.parse(f'{existing_parts[2]} EST', tzinfos=tzinfos),\n",
    "#                     'status_info_updated': None,\n",
    "#                     'last_status_change': None,\n",
    "#                     'last_w_service_disruption': None,\n",
    "                    'vendor': '',\n",
    "                    'monitor': '',\n",
    "                    'location': ''\n",
    "                }])\n",
    "\n",
    "def populateMetadataFromSourceDirName(metadata, sourceInfo):\n",
    "    if sourceInfo.startswith('cloud-amazon-web-services'):\n",
    "        metadata['vendor'] = 'AWS'\n",
    "        metadata['monitor'] = 'AWS'\n",
    "        metadata['org_type'] = 'cloud'\n",
    "    elif sourceInfo.startswith('cloud-google-apps'):\n",
    "        metadata['vendor'] = 'Google Apps'\n",
    "        metadata['monitor'] = 'Google Apps'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('cloud-google-cloud-platform'):\n",
    "        metadata['vendor'] = 'GCP'\n",
    "        metadata['monitor'] = 'GCP'\n",
    "        metadata['org_type'] = 'cloud'\n",
    "    elif sourceInfo.startswith('cloud-microsoft-azure'):\n",
    "        metadata['vendor'] = 'Azure'\n",
    "        metadata['monitor'] = 'Azure'\n",
    "        metadata['org_type'] = 'cloud'\n",
    "    elif sourceInfo.startswith('cloudflare-status'):\n",
    "        metadata['vendor'] = 'Cloudflare'\n",
    "        metadata['monitor'] = 'Cloudflare'\n",
    "        metadata['org_type'] = 'cloud'\n",
    "    elif sourceInfo.startswith('downdetector'):\n",
    "        metadata['monitor'] = 'Down Detector'\n",
    "        \n",
    "        if sourceInfo == 'downdetector':\n",
    "            metadata['location'] = 'USA'\n",
    "        elif sourceInfo.endswith('united-arab-emirates'):\n",
    "            metadata['location'] = 'UAE'\n",
    "        elif sourceInfo.endswith('argentina'):\n",
    "            metadata['location'] = 'Argentina'\n",
    "        elif sourceInfo.endswith('australia'):\n",
    "            metadata['location'] = 'Australia'\n",
    "        elif sourceInfo.endswith('austria'):\n",
    "            metadata['location'] = 'Austria'\n",
    "        elif sourceInfo.endswith('belgium'):\n",
    "            metadata['location'] = 'Belgium'\n",
    "        elif sourceInfo.endswith('brazil'):\n",
    "            metadata['location'] = 'Brazil'\n",
    "        elif sourceInfo.endswith('canada'):\n",
    "            metadata['location'] = 'Canada'\n",
    "        elif sourceInfo.endswith('switzerland'):\n",
    "            metadata['location'] = 'Switzerland'\n",
    "        elif sourceInfo.endswith('chile'):\n",
    "            metadata['location'] = 'Chile'\n",
    "        elif sourceInfo.endswith('denmark'):\n",
    "            metadata['location'] = 'Denmark'\n",
    "        elif sourceInfo.endswith('germany'):\n",
    "            metadata['location'] = 'Germany'\n",
    "        elif sourceInfo.endswith('spain'):\n",
    "            metadata['location'] = 'Spain'\n",
    "        elif sourceInfo.endswith('finland'):\n",
    "            metadata['location'] = 'Finland'\n",
    "        elif sourceInfo.endswith('france'):\n",
    "            metadata['location'] = 'France'\n",
    "        elif sourceInfo.endswith('great-britain'):\n",
    "            metadata['location'] = 'Great Britain'\n",
    "        elif sourceInfo.endswith('india'):\n",
    "            metadata['location'] = 'India'\n",
    "        elif sourceInfo.endswith('ireland'):\n",
    "            metadata['location'] = 'Ireland'\n",
    "        elif sourceInfo.endswith('italy'):\n",
    "            metadata['location'] = 'Italy'\n",
    "        elif sourceInfo.endswith('japan'):\n",
    "            metadata['location'] = 'Japan'\n",
    "        elif sourceInfo.endswith('mexico'):\n",
    "            metadata['location'] = 'Mexico'\n",
    "        elif sourceInfo.endswith('netherlands'):\n",
    "            metadata['location'] = 'Netherlands'\n",
    "        elif sourceInfo.endswith('norway'):\n",
    "            metadata['location'] = 'Norway'\n",
    "        elif sourceInfo.endswith('new-zealand'):\n",
    "            metadata['location'] = 'New Zealand'\n",
    "        elif sourceInfo.endswith('poland'):\n",
    "            metadata['location'] = 'Poland'\n",
    "        elif sourceInfo.endswith('portugal'):\n",
    "            metadata['location'] = 'Portugal'\n",
    "        elif sourceInfo.endswith('russia'):\n",
    "            metadata['location'] = 'Russia'\n",
    "        elif sourceInfo.endswith('singapore'):\n",
    "            metadata['location'] = 'Singapore'\n",
    "        elif sourceInfo.endswith('sweden'):\n",
    "            metadata['location'] = 'Sweden'\n",
    "        elif sourceInfo.endswith('south-africa'):\n",
    "            metadata['location'] = 'South Africa'\n",
    "        else:\n",
    "            raise Exception('Unknown location')\n",
    "            \n",
    "    elif sourceInfo.startswith('downrightnow'):\n",
    "        metadata['monitor'] = 'Down Right Now'\n",
    "    elif sourceInfo.startswith('github-status'):\n",
    "        metadata['vendor'] = 'Github'\n",
    "        metadata['monitor'] = 'Github'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('outage'):\n",
    "        metadata['monitor'] = 'Outage Report'\n",
    "    elif sourceInfo.startswith('cloud-apple-consumer'):\n",
    "        return False\n",
    "    elif sourceInfo.startswith('atlassian'):\n",
    "        metadata['vendor'] = 'Atlassian'\n",
    "        metadata['monitor'] = 'Atlassian'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('docker'):\n",
    "        metadata['vendor'] = 'Docker'\n",
    "        metadata['monitor'] = 'Docker'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('slack'):\n",
    "        metadata['vendor'] = 'Slack'\n",
    "        metadata['monitor'] = 'Slack'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('minecraft'):\n",
    "        metadata['vendor'] = 'Minecraft'\n",
    "        metadata['monitor'] = 'Minecraft'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('nintendo'):\n",
    "        metadata['vendor'] = 'Nintendo'\n",
    "        metadata['monitor'] = 'Nintendo'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('discord'):\n",
    "        metadata['vendor'] = 'Discord'\n",
    "        metadata['monitor'] = 'Discord'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('gpanel'):\n",
    "        return False\n",
    "    else:\n",
    "        raise Exception('Unknown data source: ' + sourceInfo)\n",
    "        \n",
    "    return True\n",
    "\n",
    "def processHourDir(oneHourDirPath, metadata):\n",
    "    # ['', 'tmp', 'tmp9_ms31tl', '20171114T000001']\n",
    "    hourInfo = oneHourDirPath.split('/')[-1][9:11]\n",
    "    metadata['hour'] = hourInfo\n",
    "    \n",
    "    return_df = None\n",
    "\n",
    "    # add parse conditions here\n",
    "    # also extract monitor source and target service from sourceinfo here\n",
    "    # data also extracted from inside the file\n",
    "    #\n",
    "    # pass dictionary to be merged inside parser\n",
    "    # then, it would be like\n",
    "    monitor_name = metadata['monitor']\n",
    "    if monitor_name == 'Down Detector':\n",
    "        ls = []\n",
    "        for filename in os.listdir(oneHourDirPath):\n",
    "            with open(os.path.join(oneHourDirPath, filename), 'r') as file_obj:\n",
    "                processed_file_df = process_downdetector_file(filename, file_obj, metadata)\n",
    "                processed_file_df['vendor'] = filename.split('.')[0]\n",
    "                ls.append(processed_file_df)\n",
    "        if len(ls) == 0:\n",
    "            return_df = None\n",
    "        else:\n",
    "            return_df = pd.concat(ls)\n",
    "    elif monitor_name == 'Down Right Now':\n",
    "#         return_df=None\n",
    "        ls = []\n",
    "        for filename in os.listdir(oneHourDirPath):\n",
    "            with open(os.path.join(oneHourDirPath, filename), 'r') as file_obj:\n",
    "                processor = DRNParser()\n",
    "                processed_file_df = processor.process_file(filename, file_obj, oneHourDirPath.split('/')[-1])\n",
    "                processed_file_df['vendor'] = filename.split('.')[0]\n",
    "                ls.append(processed_file_df)\n",
    "        if len(ls) == 0:\n",
    "            return_df = None\n",
    "        else:\n",
    "            return_df = pd.concat(ls)\n",
    "    elif monitor_name == 'Outage Report':\n",
    "#         return_df=None # for now, will get back to it later\n",
    "        ls = []\n",
    "        for filename in os.listdir(oneHourDirPath):\n",
    "            with open(os.path.join(oneHourDirPath, filename), 'r') as file_obj:\n",
    "                processor = OutageReportParser()\n",
    "                processed_file_df = processor.process_outagereport(filename, file_obj)\n",
    "                processed_file_df['vendor'] = filename.split('.')[0]\n",
    "                ls.append(processed_file_df)\n",
    "        if len(ls) == 0:\n",
    "            return_df = None\n",
    "        else:\n",
    "            return_df = pd.concat(ls)\n",
    "    \n",
    "    if return_df is None or len(return_df) == 0:\n",
    "        return_df = PLACEHOLDER_DF\n",
    "    else: \n",
    "        return_df['monitor'] = metadata['monitor']\n",
    "        if 'location' in metadata:\n",
    "            return_df['location'] = metadata['location']\n",
    "        else:\n",
    "            return_df['location'] = None\n",
    "    \n",
    "#     print(return_df.dtypes)\n",
    "#     print(return_df.loc[0, :])\n",
    "        \n",
    "    return return_df\n",
    "\n",
    "def processDayZip(oneDayZipPath, metadata):\n",
    "    # ['', 'tmp', 'tmp9_ms31tl', '20171114', 'zip']\n",
    "    dayInfo = int(pathSplitRegex.split(oneDayZipPath)[-2][6:])\n",
    "    metadata['day'] = dayInfo\n",
    "    \n",
    "    oneDayZip = zipfile.ZipFile(oneDayZipPath)\n",
    "    \n",
    "    with tempfile.TemporaryDirectory(prefix=APP_TEMP_PREFIX) as dayTempDir:\n",
    "        oneDayZip.extractall(dayTempDir)\n",
    "        hourDirPaths = map(lambda x: os.path.join(dayTempDir, x), os.listdir(dayTempDir))\n",
    "\n",
    "        return list(map(lambda oneHourDirPath: processHourDir(oneHourDirPath, metadata), hourDirPaths))\n",
    "\n",
    "def processMonthZip(squashFsImage, monthZipPath):\n",
    "    arrayOfDataFrames = []\n",
    "    \n",
    "    # Collecting info for the dataframe\n",
    "    # ['', 'cloud-amazon-web-services', '2017', '201711', 'zip']\n",
    "    try:\n",
    "        monthZipPathSplits = pathSplitRegex.split(monthZipPath)\n",
    "        sourceInfo = monthZipPathSplits[1]\n",
    "        # outage.report has a period in the name. Thus, will lead to additional splits\n",
    "        if sourceInfo.startswith('outage'):\n",
    "            yearInfo = int(monthZipPathSplits[3])\n",
    "            if yearInfo > 2020:\n",
    "                return []\n",
    "            monthInfo = int(monthZipPathSplits[4][4:])\n",
    "        else:\n",
    "            yearInfo = int(monthZipPathSplits[2])\n",
    "            if yearInfo > 2020:\n",
    "                return []\n",
    "            monthInfo = int(monthZipPathSplits[3][4:])\n",
    "    except Exception as e:\n",
    "        print(repr(Exception))\n",
    "        print(monthZipPath)\n",
    "        return []\n",
    "    \n",
    "    metadata = {\n",
    "        'year': yearInfo,\n",
    "        'month': monthInfo\n",
    "    }\n",
    "    \n",
    "    validSource = populateMetadataFromSourceDirName(metadata, sourceInfo)\n",
    "    if not validSource:\n",
    "        return []\n",
    "    \n",
    "    monthZipHandle = squashFsImage.root.select(monthZipPath)\n",
    "    with tempfile.TemporaryFile(prefix=APP_TEMP_PREFIX) as monthZipTempFile:\n",
    "        monthZipTempFile.write(monthZipHandle.getContent())\n",
    "        oneMonthZip = zipfile.ZipFile(monthZipTempFile)\n",
    "        \n",
    "        with tempfile.TemporaryDirectory(prefix=APP_TEMP_PREFIX) as monthTempDir:\n",
    "            oneMonthZip.extractall(monthTempDir)\n",
    "            dayZipPaths = map(lambda x: os.path.join(monthTempDir, x), os.listdir(monthTempDir))\n",
    "            \n",
    "            return list(itertools.chain.from_iterable(map(lambda oneDayZipPath: processDayZip(oneDayZipPath, metadata), dayZipPaths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_of_interest = 2020\n",
    "archive_path = \"/var/scratch/atlarge/traces/cloud-availability-sacheen-2020-05-11.sqsh\"\n",
    "# Install pysquashfs directly from git. The version on pypi has bug and hasn't been fixed in over 4 years.\n",
    "image = SquashFsImage(archive_path)\n",
    "zipfiles = []\n",
    "for i in image.root.findAll():\n",
    "    if not i.isFolder() and str(year_of_interest) in i.getPath():\n",
    "#         print(i.getPath())\n",
    "#         if 'amazon' in i.getPath() or 'azure' in i.getPath() or 'google' in i.getPath():\n",
    "#             zipfiles.append(i.getPath())\n",
    "        zipfiles.append(i.getPath())\n",
    "\n",
    "# There is a zipfile for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/scratch/stalluri/miniconda3/envs/thesis/lib/python3.8/site-packages/distributed/dashboard/core.py:79: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    }
   ],
   "source": [
    "cluster = SLURMCluster(cores=16, memory=\"64 GB\", processes=16,\n",
    "                       local_directory=\"./scheduler_spill\",\n",
    "                       scheduler_options={'dashboard_address': ':8787'},\n",
    "                       interface='ib0', walltime='02:00:00')\n",
    "cluster.scale_up(10)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metaProcessMonthZip(path):\n",
    "    image = SquashFsImage(archive_path)\n",
    "#     processMonthZip(image, path)\n",
    "#     return pd.DataFrame([])\n",
    "    l = processMonthZip(image, path)\n",
    "    if type(l) != list or len(l) == 0:\n",
    "#         print(path)\n",
    "        return PLACEHOLDER_DF\n",
    "    return pd.concat(l)\n",
    "\n",
    "tasks = list(map(delayed(metaProcessMonthZip), zipfiles))\n",
    "df2 = dd.from_delayed(tasks, meta={\n",
    "    'event_time': 'datetime64[ns]',\n",
    "    'status_code': np.float,\n",
    "#     'status_info_updated': 'datetime64[ns]',\n",
    "#     'last_status_change': 'datetime64[ns]',\n",
    "#     'last_w_service_disruption': 'datetime64[ns]',\n",
    "    'vendor': str,\n",
    "    'monitor': str,\n",
    "    'location': str\n",
    "}).drop_duplicates()\n",
    "# df2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/scratch/stalluri/miniconda3/envs/thesis/lib/python3.8/site-packages/fsspec/implementations/local.py:29: FutureWarning: The default value of auto_mkdir=True has been deprecated and will be changed to auto_mkdir=False by default in a future release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# fut = client.persist(df2)\n",
    "# client.recreate_error_locally(fut)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "# df2.to_parquet('/var/scratch/stalluri/crowdsourced_failures_2020_with_outagereport')\n",
    "df2.to_parquet('/var/scratch/stalluri/crowdsourced_failures_2020')\n",
    "end = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:32.936628\n"
     ]
    }
   ],
   "source": [
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "await cluster.scale_down(cluster.workers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
