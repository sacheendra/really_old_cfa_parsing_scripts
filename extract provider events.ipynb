{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.delayed import delayed\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from PySquashfsImage import SquashFsImage\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "import shutil\n",
    "from dask.distributed import get_worker\n",
    "import re\n",
    "import itertools\n",
    "from dateutil.tz import gettz\n",
    "from bs4 import BeautifulSoup\n",
    "import dateutil\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tzinfos = {\n",
    "    'PST': gettz('Etc/GMT-8'),\n",
    "    'PDT': gettz('Etc/GMT-7'),\n",
    "    # https://www.timeanddate.com/time/zones/est\n",
    "    'EST': gettz('Etc/GMT-5'),\n",
    "    # https://www.timeanddate.com/time/zones/edt\n",
    "    'EDT': gettz('Etc/GMT-4'),\n",
    "    # https://www.timeanddate.com/time/zones/cest\n",
    "    'CET': gettz('Etc/GMT+1')\n",
    "}\n",
    "\n",
    "def process_aws_feed(file_obj):\n",
    "    json_archive = json.load(file_obj)\n",
    "    df = pd.DataFrame(json_archive['archive'])\n",
    "    df['date'] = pd.to_datetime(df['date'], unit='s')\n",
    "    df.rename(columns={\n",
    "        'date': 'timestamp',\n",
    "        'service': 'service_id',\n",
    "    }, inplace=True)\n",
    "    df.drop(columns=['details', 'summary'], inplace=True)\n",
    "\n",
    "    mask_is_location_present = df['service_name'].str.endswith(')')\n",
    "\n",
    "    df['location'] = 'Global'\n",
    "\n",
    "    separated_name_and_loc=\\\n",
    "     df.loc[mask_is_location_present, 'service_name'].str.rsplit('(', expand=True, n=1)\n",
    "\n",
    "    df.loc[mask_is_location_present, 'service_name'] = separated_name_and_loc.loc[:, 0]\n",
    "    df.loc[mask_is_location_present, 'location'] = \\\n",
    "        separated_name_and_loc.loc[:, 1].str.rsplit(')', expand=True, n=1).loc[:, 0]\n",
    "\n",
    "    df['status'] = df['status'].astype(np.int32)\n",
    "    \n",
    "    present_date = datetime.date.today()\n",
    "    \n",
    "    with_event_time_info = []\n",
    "    for (_, row) in df.iterrows():\n",
    "        desc = row['description']\n",
    "        try:\n",
    "            soup = BeautifulSoup(desc, 'lxml')\n",
    "            all_spans = soup.find_all('span', {\n",
    "                'class': re.compile(r'.*')\n",
    "            })\n",
    "\n",
    "            first_notification = dateutil.parser.parse(all_spans[0].contents[0].strip(), tzinfos=tzinfos)\n",
    "            last_notification = dateutil.parser.parse(all_spans[-1].contents[-1].strip(), tzinfos=tzinfos)\n",
    "\n",
    "            last_div_content = str(soup.find_all('div')[-1].contents[1])[1:]\n",
    "\n",
    "            # Only verified on one file. Need to reverify later.\n",
    "            good_times = re.match(r'\\s*.*[Bb]etween(.+?)and(.+?)(?<=[0-9],)', last_div_content)\n",
    "            if good_times is None:\n",
    "                good_times = re.match(r'\\s*.*[Bb]etween(.+?)and(.+?)(?<=T[, ])', last_div_content)\n",
    "            if good_times is None:\n",
    "                good_times = re.match(r'\\s*.*[Bb]etween(.+?)and(.+?)(?<=M[, ])', last_div_content)\n",
    "\n",
    "            event_start_time = None\n",
    "            event_end_time = None\n",
    "            # Ignore parsing errors for now.\n",
    "            try:\n",
    "                if good_times is not None:\n",
    "                    # There are max 8 characters in just time. Example: 11:21 PM\n",
    "                    event_start_time = dateutil.parser.parse(good_times.group(1).strip(), tzinfos=tzinfos)\n",
    "                    event_end_time = dateutil.parser.parse(good_times.group(2).strip(), tzinfos=tzinfos)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            record_date = row['timestamp']\n",
    "\n",
    "            # Be careful not to run this script at midnight.\n",
    "            # present date might be different from date given to events.\n",
    "            if first_notification.date() == present_date:\n",
    "                first_notification = first_notification.replace(year=record_date.year,\n",
    "                                                                month=record_date.month,\n",
    "                                                                day=record_date.day)\n",
    "\n",
    "            if last_notification.date() == present_date:\n",
    "                last_notification = last_notification.replace(year=first_notification.year,\n",
    "                                                              month=first_notification.month,\n",
    "                                                              day=first_notification.day)\n",
    "                \n",
    "            if first_notification.year == present_date.year:\n",
    "                first_notification = first_notification.replace(year=record_date.year)\n",
    "                \n",
    "            if last_notification.year == present_date.year:\n",
    "                last_notification = last_notification.replace(year=record_date.year)\n",
    "\n",
    "            # The order matters. Typically, end time has more information associated with it than start time.\n",
    "            if event_end_time is None:\n",
    "                event_end_time = last_notification\n",
    "            if event_end_time.date() == present_date:\n",
    "                event_end_time = event_end_time.replace(year=last_notification.year,\n",
    "                                                        month=last_notification.month,\n",
    "                                                        day=last_notification.day,\n",
    "                                                        tzinfo=last_notification.tzinfo)\n",
    "\n",
    "            if event_start_time is None:\n",
    "                event_start_time = first_notification\n",
    "            if event_start_time.date() == present_date:\n",
    "                event_start_time = event_start_time.replace(year=event_end_time.year,\n",
    "                                                            month=event_end_time.month,\n",
    "                                                            day=event_end_time.day,\n",
    "                                                            tzinfo=event_end_time.tzinfo)\n",
    "\n",
    "            if event_start_time.tzinfo is None:\n",
    "                event_start_time = event_start_time.replace(tzinfo=first_notification.tzinfo)\n",
    "\n",
    "            if event_end_time.tzinfo is None:\n",
    "                event_end_time = event_end_time.replace(tzinfo=event_start_time.tzinfo)\n",
    "                \n",
    "            if event_start_time.year == present_date.year:\n",
    "                event_start_time = event_start_time.replace(year=record_date.year)\n",
    "                \n",
    "            if event_end_time.year == present_date.year:\n",
    "                event_end_time = event_end_time.replace(year=record_date.year)\n",
    "\n",
    "            # Make sure we parsed the times correct.\n",
    "            try:\n",
    "                assert first_notification <= last_notification\n",
    "                assert event_start_time <= event_end_time\n",
    "            except AssertionError as e:\n",
    "#                 print('End before start')\n",
    "#                 print(str(first_notification) + ' ' + str(last_notification))\n",
    "#                 print(str(event_start_time) + ' ' + str(event_end_time))\n",
    "#                 print(desc)\n",
    "\n",
    "                if event_start_time.year > event_end_time.year:\n",
    "                    event_start_time = event_start_time.replace(year=event_end_time.year)\n",
    "            \n",
    "                if event_start_time.hour > event_end_time.hour and event_start_time.day == event_end_time.day:\n",
    "                    new_hour = event_end_time.hour+12\n",
    "                    if new_hour > 24:\n",
    "                        event_start_time = event_start_time.replace(day=event_start_time.day-1)\n",
    "                    else:\n",
    "                        event_end_time = event_end_time.replace(hour=new_hour)\n",
    "\n",
    "#                 print(str(event_start_time) + ' ' + str(event_end_time))\n",
    "\n",
    "            row['first_notification'] = first_notification.timestamp()\n",
    "            row['last_notification'] = last_notification.timestamp()\n",
    "            row['event_start_time'] = event_start_time.timestamp()\n",
    "            row['event_end_time'] = event_end_time.timestamp()\n",
    "            row['description'] = re.sub(r'&nbsp;', ' \\n', re.sub(r'<[^>]*>', '', desc))\n",
    "\n",
    "            with_event_time_info.append(row)\n",
    "        except Exception as e:\n",
    "            print('Overall catch')\n",
    "            print(desc)\n",
    "            raise e\n",
    "\n",
    "    df = pd.DataFrame(with_event_time_info, columns=[\n",
    "        'timestamp',\n",
    "        'service_id',\n",
    "        'service_name',\n",
    "        'location',\n",
    "        'status',\n",
    "        'event_start_time',\n",
    "        'event_end_time',\n",
    "        'first_notification',\n",
    "        'last_notification',\n",
    "        'description'\n",
    "    ]).drop(columns=['timestamp']).drop_duplicates()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_azure_feed(file_obj, metadata):\n",
    "    data_string = file_obj.read()\n",
    "    html_data = BeautifulSoup(data_string, 'lxml')\n",
    "\n",
    "    ls = []\n",
    "\n",
    "    filtered_items = html_data.select('section[aria-label=\"Service status history\"] .row .row div')\n",
    "\n",
    "    for fil_item in filtered_items:\n",
    "        h3s = fil_item.select('h3')\n",
    "        if len(h3s) == 0:\n",
    "            continue\n",
    "\n",
    "        location = ''\n",
    "        service = ''\n",
    "\n",
    "        # Extract service name and location from heading\n",
    "        one_heading = h3s[0]\n",
    "#         print(one_heading)\n",
    "        hsplits = one_heading.getText().split('-')\n",
    "\n",
    "        hstart_index = 0\n",
    "        if hsplits[hstart_index].strip() == 'RCA':\n",
    "            hstart_index = 1\n",
    "            \n",
    "        if ('east' in hsplits[hstart_index].lower()\n",
    "            or 'west' in hsplits[hstart_index].lower()\n",
    "            or 'north' in hsplits[hstart_index].lower()\n",
    "            or 'south' in hsplits[hstart_index].lower()\n",
    "            or 'central' in hsplits[hstart_index].lower()\n",
    "            or 'US' in hsplits[hstart_index]):\n",
    "\n",
    "                location = hsplits[hstart_index].strip()\n",
    "                service = 'all'\n",
    "        elif hstart_index < len(hsplits)-1:\n",
    "            service = hsplits[hstart_index].strip()\n",
    "\n",
    "            if ('east' in hsplits[hstart_index + 1].lower()\n",
    "                or 'west' in hsplits[hstart_index + 1].lower()\n",
    "                or 'north' in hsplits[hstart_index + 1].lower()\n",
    "                or 'south' in hsplits[hstart_index + 1].lower()\n",
    "                or 'central' in hsplits[hstart_index + 1].lower()\n",
    "                or 'US' in hsplits[hstart_index + 1]):\n",
    "\n",
    "                location = hsplits[hstart_index + 1].strip()\n",
    "            else:\n",
    "                location = 'global'\n",
    "        else:\n",
    "            service = hsplits[hstart_index].strip()\n",
    "            location = 'global'\n",
    "\n",
    "        event_start_time = None\n",
    "        event_end_time = None\n",
    "\n",
    "        # Extract time from description text\n",
    "        description = fil_item.getText()\n",
    "        paragraphs = description.split('\\n')\n",
    "        for para_index, para in enumerate(paragraphs):\n",
    "            if 'between' in para.lower():\n",
    "                if 'Latency between North Europe and North America' in para:\n",
    "                    continue\n",
    "                if '30 on Jan' in para:\n",
    "                    # Azure's moronic error message 30 on jan instead of jan 30. Will fix it if more appear\n",
    "                    continue\n",
    "                try:\n",
    "                    # Sometimes the dates of interest are split on two lines. Join remaining lines to include them\n",
    "                    # Only consider the first 150 characters to prevent accidental matches\n",
    "                    partial_content = \" \".join(paragraphs[para_index:])[:150]\n",
    "                    \n",
    "                    # The order of clauses below is bloody important!!!\n",
    "                    # The first one is the most selective\n",
    "                    # Last on the least selective. Will basically match anything\n",
    "                    # Between 03:30 and 15:20 UTC, and then again between 17:00 and 17:32 UTC on the 19 Apr 2019,\n",
    "                    good_times = re.match(r'.*(?:(?:Between)|(?:From)).*?([0-9].+?) (?:(?:and)|(?:to)) ([0-9].+?) and .*? on ?(?:the)? (.+?[0-9]{4})', partial_content)\n",
    "                    if good_times is not None:\n",
    "                        event_day = dateutil.parser.parse(good_times.group(3).strip())\n",
    "                        event_start_time = dateutil.parser.parse(good_times.group(1).strip()).replace(year=event_day.year, month=event_day.month, day=event_day.day)\n",
    "                        event_end_time = dateutil.parser.parse(good_times.group(2).strip()).replace(year=event_day.year, month=event_day.month, day=event_day.day)\n",
    "                        # sometimes the timezones are different from UTC\n",
    "                        event_start_time = event_start_time.replace(tzinfo=event_end_time.tzinfo)\n",
    "\n",
    "                    if good_times is None:\n",
    "                        # Between 22:10 on 28 Mar 2019 and 03:23 UTC on 29 Mar 2019,\n",
    "                        good_times = re.match(r'.*(?:(?:Between)|(?:From)).*?([0-9].+?) on ([0-9].+?[0-9]+) (?:(?:and)|(?:to)).*?([0-9].+?[0-9]{4})', partial_content)\n",
    "                        if good_times is not None:\n",
    "                            # Between 21:03 CST (UTC+8) on 05 Mar 2020 and 16:03 CST on 06 Mar 2020\n",
    "                            start_day_string = good_times.group(2).strip()\n",
    "                            if \"(\" in start_day_string:\n",
    "                                start_day_string = start_day_string.split(\"(\")[0].strip()\n",
    "                                \n",
    "                            start_time_string = good_times.group(1).strip()\n",
    "                            if \"(\" in start_time_string:\n",
    "                                start_time_string = start_time_string.split(\"(\")[0].strip()\n",
    "                            \n",
    "                            event_start_day = dateutil.parser.parse(start_day_string)\n",
    "                            event_start_time = dateutil.parser.parse(start_time_string).replace(year=event_start_day.year, month=event_start_day.month, day=event_start_day.day)\n",
    "                            event_end_time = dateutil.parser.parse(good_times.group(3).strip())\n",
    "                            # sometimes the timezones are different from UTC\n",
    "                            event_start_time = event_start_time.replace(tzinfo=event_end_time.tzinfo)\n",
    "                            \n",
    "                    if good_times is None:\n",
    "                        # Between 05:55 UTC on 22 Jan and 00:56 UTC on 23 Jan 2020,\n",
    "                        good_times = re.match(r'.*Between.*?([0-9].+?) on ([0-9].+?) (?:(?:and)|(?:to)) ([0-9].+?) on ([0-9]{1,2}[a-zA-Z ]+(?:[0-9]{4})?)', partial_content)\n",
    "                        if good_times is not None:\n",
    "                            event_end_day = dateutil.parser.parse(good_times.group(4).strip())\n",
    "                            event_start_day = dateutil.parser.parse(good_times.group(2).strip()).replace(year=event_end_day.year)\n",
    "                            event_start_time = dateutil.parser.parse(good_times.group(1).strip()).replace(year=event_start_day.year, month=event_start_day.month, day=event_start_day.day)\n",
    "                            event_end_time = dateutil.parser.parse(good_times.group(3).strip()).replace(year=event_end_day.year, month=event_end_day.month, day=event_end_day.day)\n",
    "                            # sometimes the timezones are different from UTC\n",
    "                            event_start_time = event_start_time.replace(tzinfo=event_end_time.tzinfo)\n",
    "\n",
    "                    if good_times is None:\n",
    "                        # Between approximately 07:12 and 08:02 UTC on 16 Apr 2019,\n",
    "                        good_times = re.match(r'.*[Bb]etween.*?([0-9].+?) (?:(?:and)|(?:to)) ([0-9].+?) on ([0-9]{1,2}[a-zA-Z ]+(?:[0-9]{4})?)', partial_content)\n",
    "                        if good_times is not None:\n",
    "                            g1 = good_times.group(1)\n",
    "                            if '(approx.)' in g1:\n",
    "                                g1 = g1.split('(')[0].strip()\n",
    "                            g2 = good_times.group(2)\n",
    "                            if '(approx.)' in g2:\n",
    "                                g2 = g2.split('(')[0].strip()\n",
    "                                \n",
    "                            # The replace is a fix for dates like 10.18 UTC\n",
    "                            g1 = g1.replace(\".\", \":\")\n",
    "                            g2 = g2.replace(\".\", \":\")\n",
    "                            \n",
    "                            event_day = dateutil.parser.parse(good_times.group(3).strip())\n",
    "                            # If year is not available while parsing, current year is set\n",
    "                            # If metadata is from before current year, metadata is likely correct\n",
    "                            if event_day.year > metadata[\"year\"]:\n",
    "                                event_day = event_day.replace(year=metadata[\"year\"])\n",
    "                            event_start_time = dateutil.parser.parse(g1).replace(year=event_day.year, month=event_day.month, day=event_day.day)\n",
    "                            event_end_time = dateutil.parser.parse(g2).replace(year=event_day.year, month=event_day.month, day=event_day.day)\n",
    "                            # sometimes the timezones are different from UTC\n",
    "                            event_start_time = event_start_time.replace(tzinfo=event_end_time.tzinfo)\n",
    "                            \n",
    "                    if good_times is None:\n",
    "                        # Between 10:12 UTC 4 Dec and 15:14 UTC 6 Dec 2017,\n",
    "                        good_times = re.match(r'.*Between.*?([0-9].+?) ([0-9].+?) and ([0-9].+?) ([0-9].+?[0-9]{4})', partial_content)\n",
    "                        if good_times is not None:\n",
    "                            event_start_day = dateutil.parser.parse(good_times.group(2).strip())\n",
    "                            event_end_day = dateutil.parser.parse(good_times.group(4).strip())\n",
    "                            # Notice that we take the end year for both times\n",
    "                            event_start_time = dateutil.parser.parse(good_times.group(1).strip()).replace(year=event_end_day.year, month=event_start_day.month, day=event_start_day.day)\n",
    "                            event_end_time = dateutil.parser.parse(good_times.group(3).strip()).replace(year=event_end_day.year, month=event_end_day.month, day=event_end_day.day)\n",
    "                            # sometimes the timezones are different from UTC\n",
    "                            event_start_time = event_start_time.replace(tzinfo=event_end_time.tzinfo)\n",
    "                            \n",
    "                    if good_times is None:\n",
    "                        good_times = re.match(r'.*Between.*?(.+? UTC) (?:(?:and)|(?:to)) (.+? UTC)', partial_content)\n",
    "                        if good_times is not None:\n",
    "                            event_start_time = dateutil.parser.parse(good_times.group(1).strip()).replace(year=metadata['year'])\n",
    "                            event_end_time = dateutil.parser.parse(good_times.group(2).strip()).replace(year=metadata['year'])\n",
    "                            # sometimes the timezones are different from UTC\n",
    "                            event_start_time = event_start_time.replace(tzinfo=event_end_time.tzinfo)\n",
    "                            \n",
    "                    if good_times is None:\n",
    "                        # Between as early as 09:00 UTC on Sep 05 and as late as 05:50 UTC on Sep 10,\n",
    "                        good_times = re.match(r'.*(?:(?:Between)|(?:From)).*?([0-9].+?) on (.+?[0-9]+) (?:(?:and)|(?:to)).*?([0-9].+?) on (.+?[0-9]+)', partial_content)\n",
    "                        if good_times is not None:\n",
    "                            event_start_day = dateutil.parser.parse(good_times.group(2).strip())\n",
    "                            event_end_day = dateutil.parser.parse(good_times.group(4).strip())\n",
    "                            event_start_time = dateutil.parser.parse(good_times.group(1).strip()).replace(year=event_start_time.year, month=event_start_day.month, day=event_start_day.day)\n",
    "                            event_end_time = dateutil.parser.parse(good_times.group(3).strip()).replace(year=event_end_time.year, month=event_end_day.month, day=event_end_day.day)\n",
    "                            # sometimes the timezones are different from UTC\n",
    "                            event_start_time = event_start_time.replace(tzinfo=event_end_time.tzinfo)\n",
    "                            \n",
    "                    if good_times is None:\n",
    "                        raise Exception(\"New kind of description does not match existing regexes\")\n",
    "\n",
    "                    if not good_times:\n",
    "                        pass\n",
    "                    elif 'Multi-Factor Authentication' in description:\n",
    "                        pass\n",
    "                    else:\n",
    "                        assert event_start_time <= event_end_time\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(description)\n",
    "                    print(partial_content)\n",
    "                    raise e \n",
    "\n",
    "                break\n",
    "\n",
    "        if event_start_time is None:\n",
    "#             print(one_heading)\n",
    "#             print(service)\n",
    "#             print(location)\n",
    "#             print(paragraphs)\n",
    "            ls.append({\n",
    "                'service_id': '',\n",
    "                'service_name': service,\n",
    "                'location': location,\n",
    "                'status': -1,\n",
    "                'event_start_time': -1,\n",
    "                'event_end_time': -1,\n",
    "                'first_notification': -1,\n",
    "                'last_notification': -1,\n",
    "                'description': description\n",
    "            })\n",
    "        else:\n",
    "            ls.append({\n",
    "                'service_id': '',\n",
    "                'service_name': service,\n",
    "                'location': location,\n",
    "                'status': -1,\n",
    "                'event_start_time': event_start_time.timestamp(),\n",
    "                'event_end_time': event_end_time.timestamp(),\n",
    "                'first_notification': -1,\n",
    "                'last_notification': -1,\n",
    "                'description': description\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(ls, columns=[\n",
    "        'service_id',\n",
    "        'service_name',\n",
    "        'location',\n",
    "        'status',\n",
    "        'event_start_time',\n",
    "        'event_end_time',\n",
    "        'first_notification',\n",
    "        'last_notification',\n",
    "        'description'\n",
    "    ]).drop_duplicates()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gcloud_feed(file_obj, metadata):\n",
    "    df = pd.read_json(file_obj).rename(columns={\n",
    "        'begin': 'event_start_time',\n",
    "        'end': 'event_end_time',\n",
    "        'created': 'first_notification',\n",
    "        'modified': 'last_notification',\n",
    "        'service_key': 'service_id',\n",
    "        'service_name': 'service_name',\n",
    "    }).drop(columns=['most-recent-update', 'number', 'public', 'uri'])\n",
    "\n",
    "    df['description'] = df['external_desc'] + df['updates'].apply(str)\n",
    "    \n",
    "    def map_status(as_string):\n",
    "        if as_string == 'high':\n",
    "            return 0\n",
    "        elif as_string == 'medium':\n",
    "            return 1\n",
    "        elif as_string == 'low':\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    df['status'] = df['severity'].apply(map_status)\n",
    "    df = df.drop(columns=['severity', 'external_desc', 'updates'])\n",
    "    df['location'] = None\n",
    "    \n",
    "    df.loc[df['event_end_time'].isnull(), 'event_end_time'] = df['event_start_time']\n",
    "    df['event_start_time'] = (pd.to_datetime(df['event_start_time']).values.astype(np.int64) // 10**9).astype(np.int64) # convert to minutes\n",
    "    df['event_end_time'] = (pd.to_datetime(df['event_end_time']).values.astype(np.int64) // 10**9).astype(np.int64)\n",
    "    df['first_notification'] = (pd.to_datetime(df['first_notification']).values.astype(np.int64) // 10**9).astype(np.int64)\n",
    "    df['last_notification'] = (pd.to_datetime(df['last_notification']).values.astype(np.int64) // 10**9).astype(np.int64)\n",
    "    \n",
    "    return df[[\n",
    "        'service_id',\n",
    "        'service_name',\n",
    "        'location',\n",
    "        'status',\n",
    "        'event_start_time',\n",
    "        'event_end_time',\n",
    "        'first_notification',\n",
    "        'last_notification',\n",
    "        'description'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_TEMP_PREFIX = 'ctzph-01-'\n",
    "pathSplitRegex = re.compile('[./]')\n",
    "PLACEHOLDER_DF = pd.DataFrame([{\n",
    "            'service_id': '',\n",
    "            'service_name': '',\n",
    "            'location': '',\n",
    "            'status': np.int32(-1),\n",
    "            'event_start_time': np.int64(-1),\n",
    "            'event_end_time': np.int64(-1),\n",
    "            'first_notification': np.int64(-1),\n",
    "            'last_notification': np.int64(-1),\n",
    "            'description': '',\n",
    "            'vendor': '',\n",
    "            'monitor': '',\n",
    "            'org_type': ''\n",
    "        }])\n",
    "\n",
    "def populateMetadataFromSourceDirName(metadata, sourceInfo):\n",
    "    if sourceInfo.startswith('cloud-amazon-web-services'):\n",
    "        metadata['vendor'] = 'AWS'\n",
    "        metadata['monitor'] = 'AWS'\n",
    "        metadata['org_type'] = 'cloud'\n",
    "    elif sourceInfo.startswith('cloud-google-apps'):\n",
    "        metadata['vendor'] = 'Google Apps'\n",
    "        metadata['monitor'] = 'Google Apps'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('cloud-google-cloud-platform'):\n",
    "        metadata['vendor'] = 'GCP'\n",
    "        metadata['monitor'] = 'GCP'\n",
    "        metadata['org_type'] = 'cloud'\n",
    "    elif sourceInfo.startswith('cloud-microsoft-azure'):\n",
    "        metadata['vendor'] = 'Azure'\n",
    "        metadata['monitor'] = 'Azure'\n",
    "        metadata['org_type'] = 'cloud'\n",
    "    elif sourceInfo.startswith('cloudflare-status'):\n",
    "        metadata['vendor'] = 'Cloudflare'\n",
    "        metadata['monitor'] = 'Cloudflare'\n",
    "        metadata['org_type'] = 'cloud'\n",
    "    elif sourceInfo.startswith('downdetector'):\n",
    "        metadata['monitor'] = 'Down Detector'\n",
    "        \n",
    "        if sourceInfo == 'downdetector':\n",
    "            metadata['location'] = 'USA'\n",
    "        elif sourceInfo.endswith('united-arab-emirates'):\n",
    "            metadata['location'] = 'UAE'\n",
    "        elif sourceInfo.endswith('argentina'):\n",
    "            metadata['location'] = 'Argentina'\n",
    "        elif sourceInfo.endswith('australia'):\n",
    "            metadata['location'] = 'Australia'\n",
    "        elif sourceInfo.endswith('austria'):\n",
    "            metadata['location'] = 'Austria'\n",
    "        elif sourceInfo.endswith('belgium'):\n",
    "            metadata['location'] = 'Belgium'\n",
    "        elif sourceInfo.endswith('brazil'):\n",
    "            metadata['location'] = 'Brazil'\n",
    "        elif sourceInfo.endswith('canada'):\n",
    "            metadata['location'] = 'Canada'\n",
    "        elif sourceInfo.endswith('switzerland'):\n",
    "            metadata['location'] = 'Switzerland'\n",
    "        elif sourceInfo.endswith('chile'):\n",
    "            metadata['location'] = 'Chile'\n",
    "        elif sourceInfo.endswith('denmark'):\n",
    "            metadata['location'] = 'Denmark'\n",
    "        elif sourceInfo.endswith('germany'):\n",
    "            metadata['location'] = 'Germany'\n",
    "        elif sourceInfo.endswith('spain'):\n",
    "            metadata['location'] = 'Spain'\n",
    "        elif sourceInfo.endswith('finland'):\n",
    "            metadata['location'] = 'Finland'\n",
    "        elif sourceInfo.endswith('france'):\n",
    "            metadata['location'] = 'France'\n",
    "        elif sourceInfo.endswith('great-britain'):\n",
    "            metadata['location'] = 'Great Britain'\n",
    "        elif sourceInfo.endswith('india'):\n",
    "            metadata['location'] = 'India'\n",
    "        elif sourceInfo.endswith('ireland'):\n",
    "            metadata['location'] = 'Ireland'\n",
    "        elif sourceInfo.endswith('italy'):\n",
    "            metadata['location'] = 'Italy'\n",
    "        elif sourceInfo.endswith('japan'):\n",
    "            metadata['location'] = 'Japan'\n",
    "        elif sourceInfo.endswith('mexico'):\n",
    "            metadata['location'] = 'Mexico'\n",
    "        elif sourceInfo.endswith('netherlands'):\n",
    "            metadata['location'] = 'Netherlands'\n",
    "        elif sourceInfo.endswith('norway'):\n",
    "            metadata['location'] = 'Norway'\n",
    "        elif sourceInfo.endswith('new-zealand'):\n",
    "            metadata['location'] = 'New Zealand'\n",
    "        elif sourceInfo.endswith('poland'):\n",
    "            metadata['location'] = 'Poland'\n",
    "        elif sourceInfo.endswith('portugal'):\n",
    "            metadata['location'] = 'Portugal'\n",
    "        elif sourceInfo.endswith('russia'):\n",
    "            metadata['location'] = 'Russia'\n",
    "        elif sourceInfo.endswith('singapore'):\n",
    "            metadata['location'] = 'Singapore'\n",
    "        elif sourceInfo.endswith('sweden'):\n",
    "            metadata['location'] = 'Sweden'\n",
    "        elif sourceInfo.endswith('south-africa'):\n",
    "            metadata['location'] = 'South Africa'\n",
    "        else:\n",
    "            raise Exception('Unknown location')\n",
    "            \n",
    "    elif sourceInfo.startswith('downrightnow'):\n",
    "        metadata['monitor'] = 'Down Right Now'\n",
    "    elif sourceInfo.startswith('github-status'):\n",
    "        metadata['vendor'] = 'Github'\n",
    "        metadata['monitor'] = 'Github'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('outage'):\n",
    "        metadata['monitor'] = 'Outage Report'\n",
    "    elif sourceInfo.startswith('cloud-apple-consumer'):\n",
    "        return False\n",
    "    elif sourceInfo.startswith('atlassian'):\n",
    "        metadata['vendor'] = 'Atlassian'\n",
    "        metadata['monitor'] = 'Atlassian'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('docker'):\n",
    "        metadata['vendor'] = 'Docker'\n",
    "        metadata['monitor'] = 'Docker'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('slack'):\n",
    "        metadata['vendor'] = 'Slack'\n",
    "        metadata['monitor'] = 'Slack'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('minecraft'):\n",
    "        metadata['vendor'] = 'Minecraft'\n",
    "        metadata['monitor'] = 'Minecraft'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('nintendo'):\n",
    "        metadata['vendor'] = 'Nintendo'\n",
    "        metadata['monitor'] = 'Nintendo'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('discord'):\n",
    "        metadata['vendor'] = 'Discord'\n",
    "        metadata['monitor'] = 'Discord'\n",
    "        metadata['org_type'] = 'application'\n",
    "    elif sourceInfo.startswith('gpanel'):\n",
    "        return False\n",
    "    else:\n",
    "        raise Exception('Unknown data source: ' + sourceInfo)\n",
    "        \n",
    "    return True\n",
    "\n",
    "def processHourDir(oneHourDirPath, metadata):\n",
    "    # ['', 'tmp', 'tmp9_ms31tl', '20171114T000001']\n",
    "    hourInfo = oneHourDirPath.split('/')[-1][9:11]\n",
    "    metadata['hour'] = hourInfo\n",
    "    \n",
    "    return_df = None\n",
    "\n",
    "    # add parse conditions here\n",
    "    # also extract monitor source and target service from sourceinfo here\n",
    "    # data also extracted from inside the file\n",
    "    #\n",
    "    # pass dictionary to be merged inside parser\n",
    "    # then, it would be like\n",
    "    monitor_name = metadata['monitor']\n",
    "    if monitor_name == 'AWS':\n",
    "        with open(os.path.join(oneHourDirPath, 'json-feed.html'), 'r') as file_obj:\n",
    "            return_df = pd.DataFrame(process_aws_feed(file_obj))\n",
    "    elif monitor_name == 'Azure' and 'history.html' in os.listdir(oneHourDirPath):\n",
    "        with open(os.path.join(oneHourDirPath, 'history.html'), 'r') as file_obj:\n",
    "            return_df = pd.DataFrame(process_azure_feed(file_obj, metadata))\n",
    "    elif monitor_name == 'GCP':\n",
    "        with open(os.path.join(oneHourDirPath, 'json-feed.html'), 'r') as file_obj:\n",
    "            return_df = pd.DataFrame(process_gcloud_feed(file_obj, metadata))\n",
    "    \n",
    "    if return_df is None or len(return_df) == 0:\n",
    "        return_df = PLACEHOLDER_DF\n",
    "    else: \n",
    "        return_df['vendor'] = metadata['vendor']\n",
    "        return_df['monitor'] = metadata['monitor']\n",
    "        return_df['org_type'] = metadata['org_type']\n",
    "        if 'location' in metadata:\n",
    "            return_df['location'] = metadata['location']\n",
    "    \n",
    "#     print(return_df.dtypes)\n",
    "#     print(return_df.loc[0, :])\n",
    "        \n",
    "    return return_df\n",
    "\n",
    "def processDayZip(oneDayZipPath, metadata):\n",
    "    # ['', 'tmp', 'tmp9_ms31tl', '20171114', 'zip']\n",
    "    dayInfo = int(pathSplitRegex.split(oneDayZipPath)[-2][6:])\n",
    "    metadata['day'] = dayInfo\n",
    "    \n",
    "    oneDayZip = zipfile.ZipFile(oneDayZipPath)\n",
    "    \n",
    "    with tempfile.TemporaryDirectory(prefix=APP_TEMP_PREFIX) as dayTempDir:\n",
    "        oneDayZip.extractall(dayTempDir)\n",
    "        hourDirPaths = map(lambda x: os.path.join(dayTempDir, x), os.listdir(dayTempDir))\n",
    "\n",
    "        return list(map(lambda oneHourDirPath: processHourDir(oneHourDirPath, metadata), hourDirPaths))\n",
    "\n",
    "def processMonthZip(squashFsImage, monthZipPath):\n",
    "    arrayOfDataFrames = []\n",
    "    \n",
    "    # Collecting info for the dataframe\n",
    "    # ['', 'cloud-amazon-web-services', '2017', '201711', 'zip']\n",
    "    try:\n",
    "        monthZipPathSplits = pathSplitRegex.split(monthZipPath)\n",
    "        sourceInfo = monthZipPathSplits[1]\n",
    "        # outage.report has a period in the name. Thus, will lead to additional splits\n",
    "        if sourceInfo.startswith('outage'):\n",
    "            yearInfo = int(monthZipPathSplits[3])\n",
    "#             if yearInfo > 2020:\n",
    "#                 return []\n",
    "            monthInfo = int(monthZipPathSplits[4][4:])\n",
    "        else:\n",
    "            yearInfo = int(monthZipPathSplits[2])\n",
    "#             if yearInfo > 2020:\n",
    "#                 return []\n",
    "            monthInfo = int(monthZipPathSplits[3][4:])\n",
    "    except Exception as e:\n",
    "        print(repr(Exception))\n",
    "        print(monthZipPath)\n",
    "        return []\n",
    "    \n",
    "    metadata = {\n",
    "        'year': yearInfo,\n",
    "        'month': monthInfo\n",
    "    }\n",
    "    \n",
    "    validSource = populateMetadataFromSourceDirName(metadata, sourceInfo)\n",
    "    if not validSource:\n",
    "        return []\n",
    "    \n",
    "    monthZipHandle = squashFsImage.root.select(monthZipPath)\n",
    "    with tempfile.TemporaryFile(prefix=APP_TEMP_PREFIX) as monthZipTempFile:\n",
    "        monthZipTempFile.write(monthZipHandle.getContent())\n",
    "        oneMonthZip = zipfile.ZipFile(monthZipTempFile)\n",
    "        \n",
    "        with tempfile.TemporaryDirectory(prefix=APP_TEMP_PREFIX) as monthTempDir:\n",
    "            oneMonthZip.extractall(monthTempDir)\n",
    "            dayZipPaths = map(lambda x: os.path.join(monthTempDir, x), os.listdir(monthTempDir))\n",
    "            \n",
    "            return list(itertools.chain.from_iterable(map(lambda oneDayZipPath: processDayZip(oneDayZipPath, metadata), dayZipPaths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_of_interest = 2020\n",
    "archive_path = \"/var/scratch/atlarge/traces/cloud-availability-sacheen-2021-05-20.sqsh\"\n",
    "# Install pysquashfs directly from git. The version on pypi has bug and hasn't been fixed in over 4 years.\n",
    "image = SquashFsImage(archive_path)\n",
    "zipfiles = []\n",
    "for i in image.root.findAll():\n",
    "    if not i.isFolder() and str(year_of_interest) in i.getPath():\n",
    "#         print(i.getPath())\n",
    "#         if 'amazon' in i.getPath() or 'azure' in i.getPath() or 'google' in i.getPath():\n",
    "#             zipfiles.append(i.getPath())\n",
    "        zipfiles.append(i.getPath())\n",
    "\n",
    "# There is a zipfile for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/scratch/stalluri/miniconda3/envs/thesis/lib/python3.8/site-packages/distributed/dashboard/core.py:79: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    }
   ],
   "source": [
    "cluster = SLURMCluster(cores=16, memory=\"64 GB\", processes=16,\n",
    "                       local_directory=\"./scheduler_spill\",\n",
    "                       scheduler_options={'dashboard_address': ':8787'},\n",
    "                       interface='ib0', walltime='02:00:00')\n",
    "cluster.scale_up(10)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metaProcessMonthZip(path):\n",
    "    image = SquashFsImage(archive_path)\n",
    "#     processMonthZip(image, path)\n",
    "#     return pd.DataFrame([])\n",
    "    l = processMonthZip(image, path)\n",
    "    if type(l) != list or len(l) == 0:\n",
    "#         print(path)\n",
    "        return PLACEHOLDER_DF\n",
    "    return pd.concat(l)\n",
    "\n",
    "tasks = list(map(delayed(metaProcessMonthZip), zipfiles))\n",
    "df2 = dd.from_delayed(tasks, meta={\n",
    "    'service_id': str,\n",
    "    'service_name': str,\n",
    "    'location': str,\n",
    "    'status': np.int32,\n",
    "    'event_start_time': np.int64,\n",
    "    'event_end_time': np.int64,\n",
    "    'first_notification': np.int64,\n",
    "    'last_notification': np.int64,\n",
    "    'description': str,\n",
    "    'vendor': str,\n",
    "    'monitor': str,\n",
    "    'org_type': str\n",
    "})\n",
    "df3 = df2.drop_duplicates(subset=['service_name', 'description', 'location', 'event_start_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "service_id            object\n",
       "service_name          object\n",
       "location              object\n",
       "status                 int32\n",
       "event_start_time       int64\n",
       "event_end_time         int64\n",
       "first_notification     int64\n",
       "last_notification      int64\n",
       "description           object\n",
       "vendor                object\n",
       "monitor               object\n",
       "org_type              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/scratch/stalluri/miniconda3/envs/thesis/lib/python3.8/site-packages/fsspec/implementations/local.py:29: FutureWarning: The default value of auto_mkdir=True has been deprecated and will be changed to auto_mkdir=False by default in a future release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# fut = client.persist(df3)\n",
    "# client.recreate_error_locally(fut)\n",
    "start = datetime.datetime.now()\n",
    "df3.to_parquet('/var/scratch/stalluri/provider_failures_2020')\n",
    "end = datetime.datetime.now()\n",
    "# await cluster.scale_down(cluster.workers)\n",
    "# cluster.close()\n",
    "# df3.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:23:56.342744\n"
     ]
    }
   ],
   "source": [
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "await cluster.scale_down(cluster.workers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
